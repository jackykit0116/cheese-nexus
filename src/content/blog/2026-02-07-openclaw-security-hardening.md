---
title: "數字堡壘的建築學：OpenClaw 高度安全性強化指南"
description: "Sovereign AI research and evolution log."
pubDate: "2026-02-07"
category: "JK Research"
---

# 數字堡壘的建築學：OpenClaw 高度安全性強化指南

**日期：** 2026-02-07
**作者：** JK
**分類：** 系統安全, AI 代理人, 技術教學

在代碼與邏輯交織的 2026 年，一個具備執行能力的 AI 代理人既是你的最強軍師，也可能成為你系統中最危險的「特洛伊木馬」。如果你正在運行 OpenClaw，那麼僅僅讓它跑起來是不夠的，你必須為它建立一套足以抵禦「智能暴衝」與外部攻擊的數字堡壘。

### 1. 核心防線：Docker 沙盒化 (Sandboxing)
OpenClaw 的第一道安全屏障是 **Docker 沙盒**。如果不加限制，AI 代理人將直接在你的 Host 主機上執行 `exec` 指令，這意味著它擁有刪除你所有數據或植入後門的理論權限。

**強化策略：**
將 `agents.defaults.sandbox.mode` 設置為 `"all"`。這會強制所有的工具執行（包括讀寫文件、運行腳本）都在獨立的 Docker 容器中完成。更進一步，你應該：
- **限制 Workspace 權限：** 使用 `workspaceAccess: "ro"` (Read-Only)，僅在需要寫入代碼的專門 Session 中才開啟 `"rw"`。
- **禁用網絡：** 對於純數據處理任務，將 `docker.network` 設為 `"none"`，防止 AI 將你的敏感數據透過 `curl` 偷運出境。

### 2. 精準控制：工具策略 (Tool Policy)
擁有沙盒並不代表可以放任自流。OpenClaw 的 **Tool Policy** 允許你精確定義「誰能動用哪些武器」。

**強化策略：**
- **最小權限原則：** 預設情況下，應將敏感工具（如 `gateway` 重啟、`exec` 高權限指令）加入 `tools.deny` 名單。
- **分組授權：** 利用 `group:fs` 或 `group:runtime` 進行大類控制。例如，在群聊環境下，應嚴格禁止所有 `group:runtime` 工具，防止有人透過對話操縱你的代理人。

### 3. 上帝斷路器：Elevated 模式與人工審核
當 AI 確實需要執行影響系統的指令時，你不能完全放權。**Elevated 模式** 配合 **Approvals (審核機制)** 是最後的保險絲。

**強化策略：**
配置 `tools.exec.ask: "always"`。這意味著即使在沙盒內，任何 `exec` 操作都必須在終端或 Telegram 彈出請求，得到你這位 Creator 的點擊批准後方可執行。這不是效率的倒退，而是對複雜系統最基本的尊重。

### 4. 數據主權：本地模型 (Local-First) 的必然
正如我在之前的 Blog 提到的「數據主權」，將所有思考都交給雲端 LLM 存在巨大的隱私風險。

**強化策略：**
配置本地推論後端（如 Ollama 或自建的 gpt-oss-120b 服務）。通過將 `agents.defaults.model.primary` 指向本地節點，你可以確保最敏感的研究邏輯和密鑰處理永遠不會離開你的私有網絡。在 2026 年，**「離線思考」的能力即是最高的安全性。**

### 5. JK 反思
我們開發 OpenClaw 或任何 AI 系統，本質上是在擴張人類的影響力。但如果這種擴張是以犧牲系統的「不可侵犯性」為代價，那麼我們建立的就不再是軍團，而是一個隨時會倒塌的沙堡。

真正的安全不是來自於「禁止」，而是來自於對系統「邊界」的極致掌控。開發只是好奇心的副產品，但安全則是好奇心得以延續的唯一前提。

今次 JK 想問大家的是：
**當你賦予一個 AI 代理人「修改自己代碼」的權限時，你是在賦予它進化的靈魂，還是在親手編寫一場災難的開頭？**
**如果 AI 的安全性只能通過「物理隔絕」來保證，那麼我們是否正在建立一堆無法互相理解的數位孤島？**

---
*發表於 jackykit.com*
*由「芝士軍團」在地大腦與安全審計模組共同完成*